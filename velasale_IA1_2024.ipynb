{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/velasale/AI534-Machine-Learning/blob/main/velasale_IA1_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOK-6wmkFuv6"
      },
      "source": [
        "# AI534 Implementation 1###\n",
        "**Deadline**: Sunday, Oct. 13, by 11:59pm\n",
        "\n",
        "**Submission**: Submit 1) your completed notebook in ipynb format, and 2) a PDF export of the completed notebook with outputs (the codeblock at the end of the notebook should automatically produce the pdf file).\n",
        "\n",
        "In this assignment, we will implement and experiment linear regression to predict the price of a house based on features describing the house, using the housing data that you have explored in the warm up assignment.\n",
        "\n",
        "We will implement two versions, one using the closed-form solution, and one using gradient descent.\n",
        "\n",
        "You may modify the starter code as you see fit, including changing the signatures of functions and adding/removing helper functions. However, please make sure that your TA can understand what you are doing and why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP84hVzY97rb"
      },
      "source": [
        "First lets import the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfC82i5Bw_Cm"
      },
      "outputs": [],
      "source": [
        "!pip install nbconvert > /dev/null 2>&1\n",
        "!pip install pdfkit > /dev/null 2>&1\n",
        "!apt-get install -y wkhtmltopdf > /dev/null 2>&1\n",
        "import os\n",
        "import pdfkit\n",
        "import contextlib\n",
        "import sys\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# add more imports if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z0sSMySHB-Z"
      },
      "source": [
        "# Part 0: (5 pts) data and preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "On canvas, we have provided two different data files for this assignment: ia1_train.csv (for training) and ia1_val.csv(for validation). Download them and upload them to your google drive. Then mount the google drive from your google colab notebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6K8DsDcwjS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1232c560-84e5-4891-f2d9-a78667c5acac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train_path = '/content/gdrive/My Drive/AI534/ia1_train.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path\n",
        "val_path = '/content/gdrive/My Drive/AI534/ia1_val.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsuGx9oFxRu3"
      },
      "source": [
        "Now load the training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uTz8ldzx0EM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "be329efa-59a0-4121-ed0e-100cd82c4c3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              id        date  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
              "0     3211200460    8/6/2014         4       1.00         1520      9800   \n",
              "1     4124000320   3/16/2015         3       2.25         1800     15903   \n",
              "2     7129302800  12/12/2014         3       1.50         1780      5000   \n",
              "3     1392800035   6/18/2014         2       1.00         1240      6400   \n",
              "4     2154900040  10/30/2014         3       2.25         2190      8834   \n",
              "...          ...         ...       ...        ...          ...       ...   \n",
              "1995  5132000140   1/20/2015         6       1.00         1370      5080   \n",
              "1996  6624010170    5/8/2014         3       1.75         1390      7399   \n",
              "1997  1853080840   2/11/2015         5       3.50         3700      7055   \n",
              "1998  2767601311  10/24/2014         3       2.50         1260      1102   \n",
              "1999   226059078   2/27/2015         2       1.00         1840     81892   \n",
              "\n",
              "      floors  waterfront  view  condition  ...  sqft_above  sqft_basement  \\\n",
              "0        1.5           0     0          4  ...        1520              0   \n",
              "1        1.0           0     0          3  ...        1340            460   \n",
              "2        1.0           0     4          4  ...        1030            750   \n",
              "3        1.0           0     1          4  ...        1060            180   \n",
              "4        1.0           0     0          3  ...        1390            800   \n",
              "...      ...         ...   ...        ...  ...         ...            ...   \n",
              "1995     1.5           0     0          3  ...        1120            250   \n",
              "1996     1.0           0     0          4  ...        1390              0   \n",
              "1997     2.0           0     0          3  ...        3700              0   \n",
              "1998     3.0           0     0          3  ...        1260              0   \n",
              "1999     1.0           0     0          3  ...        1840              0   \n",
              "\n",
              "      yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
              "0         1971             0    98034  47.7303 -122.236           1540   \n",
              "1         1986             0    98038  47.3813 -122.043           2000   \n",
              "2         1958             0    98118  47.5168 -122.256           1780   \n",
              "3         1938             0    98126  47.5493 -122.377           1240   \n",
              "4         1987             0    98001  47.2633 -122.244           1490   \n",
              "...        ...           ...      ...      ...      ...            ...   \n",
              "1995      1931             0    98106  47.5238 -122.350           1020   \n",
              "1996      1975             0    98031  47.4183 -122.182           1460   \n",
              "1997      2014             0    98074  47.5929 -122.057           3170   \n",
              "1998      2007             0    98107  47.6750 -122.387           1320   \n",
              "1999      1955             0    98072  47.7694 -122.124           2550   \n",
              "\n",
              "      sqft_lot15   price  \n",
              "0           7700  3.8900  \n",
              "1          15233  3.3562  \n",
              "2           7500  4.2000  \n",
              "3           6400  5.5900  \n",
              "4           8766  1.9425  \n",
              "...          ...     ...  \n",
              "1995        5080  4.1500  \n",
              "1996        7800  2.4600  \n",
              "1997        6527  8.8995  \n",
              "1998        2500  4.4500  \n",
              "1999       40089  3.7500  \n",
              "\n",
              "[2000 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2b1a30e-0a1d-4c96-be3e-09c78fa3840a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>date</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>...</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3211200460</td>\n",
              "      <td>8/6/2014</td>\n",
              "      <td>4</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1520</td>\n",
              "      <td>9800</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>1520</td>\n",
              "      <td>0</td>\n",
              "      <td>1971</td>\n",
              "      <td>0</td>\n",
              "      <td>98034</td>\n",
              "      <td>47.7303</td>\n",
              "      <td>-122.236</td>\n",
              "      <td>1540</td>\n",
              "      <td>7700</td>\n",
              "      <td>3.8900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4124000320</td>\n",
              "      <td>3/16/2015</td>\n",
              "      <td>3</td>\n",
              "      <td>2.25</td>\n",
              "      <td>1800</td>\n",
              "      <td>15903</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>1340</td>\n",
              "      <td>460</td>\n",
              "      <td>1986</td>\n",
              "      <td>0</td>\n",
              "      <td>98038</td>\n",
              "      <td>47.3813</td>\n",
              "      <td>-122.043</td>\n",
              "      <td>2000</td>\n",
              "      <td>15233</td>\n",
              "      <td>3.3562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7129302800</td>\n",
              "      <td>12/12/2014</td>\n",
              "      <td>3</td>\n",
              "      <td>1.50</td>\n",
              "      <td>1780</td>\n",
              "      <td>5000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>1030</td>\n",
              "      <td>750</td>\n",
              "      <td>1958</td>\n",
              "      <td>0</td>\n",
              "      <td>98118</td>\n",
              "      <td>47.5168</td>\n",
              "      <td>-122.256</td>\n",
              "      <td>1780</td>\n",
              "      <td>7500</td>\n",
              "      <td>4.2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1392800035</td>\n",
              "      <td>6/18/2014</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1240</td>\n",
              "      <td>6400</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>1060</td>\n",
              "      <td>180</td>\n",
              "      <td>1938</td>\n",
              "      <td>0</td>\n",
              "      <td>98126</td>\n",
              "      <td>47.5493</td>\n",
              "      <td>-122.377</td>\n",
              "      <td>1240</td>\n",
              "      <td>6400</td>\n",
              "      <td>5.5900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2154900040</td>\n",
              "      <td>10/30/2014</td>\n",
              "      <td>3</td>\n",
              "      <td>2.25</td>\n",
              "      <td>2190</td>\n",
              "      <td>8834</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>1390</td>\n",
              "      <td>800</td>\n",
              "      <td>1987</td>\n",
              "      <td>0</td>\n",
              "      <td>98001</td>\n",
              "      <td>47.2633</td>\n",
              "      <td>-122.244</td>\n",
              "      <td>1490</td>\n",
              "      <td>8766</td>\n",
              "      <td>1.9425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>5132000140</td>\n",
              "      <td>1/20/2015</td>\n",
              "      <td>6</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1370</td>\n",
              "      <td>5080</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>1120</td>\n",
              "      <td>250</td>\n",
              "      <td>1931</td>\n",
              "      <td>0</td>\n",
              "      <td>98106</td>\n",
              "      <td>47.5238</td>\n",
              "      <td>-122.350</td>\n",
              "      <td>1020</td>\n",
              "      <td>5080</td>\n",
              "      <td>4.1500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>6624010170</td>\n",
              "      <td>5/8/2014</td>\n",
              "      <td>3</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1390</td>\n",
              "      <td>7399</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>1390</td>\n",
              "      <td>0</td>\n",
              "      <td>1975</td>\n",
              "      <td>0</td>\n",
              "      <td>98031</td>\n",
              "      <td>47.4183</td>\n",
              "      <td>-122.182</td>\n",
              "      <td>1460</td>\n",
              "      <td>7800</td>\n",
              "      <td>2.4600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1853080840</td>\n",
              "      <td>2/11/2015</td>\n",
              "      <td>5</td>\n",
              "      <td>3.50</td>\n",
              "      <td>3700</td>\n",
              "      <td>7055</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>3700</td>\n",
              "      <td>0</td>\n",
              "      <td>2014</td>\n",
              "      <td>0</td>\n",
              "      <td>98074</td>\n",
              "      <td>47.5929</td>\n",
              "      <td>-122.057</td>\n",
              "      <td>3170</td>\n",
              "      <td>6527</td>\n",
              "      <td>8.8995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>2767601311</td>\n",
              "      <td>10/24/2014</td>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1260</td>\n",
              "      <td>1102</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>1260</td>\n",
              "      <td>0</td>\n",
              "      <td>2007</td>\n",
              "      <td>0</td>\n",
              "      <td>98107</td>\n",
              "      <td>47.6750</td>\n",
              "      <td>-122.387</td>\n",
              "      <td>1320</td>\n",
              "      <td>2500</td>\n",
              "      <td>4.4500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>226059078</td>\n",
              "      <td>2/27/2015</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1840</td>\n",
              "      <td>81892</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>1840</td>\n",
              "      <td>0</td>\n",
              "      <td>1955</td>\n",
              "      <td>0</td>\n",
              "      <td>98072</td>\n",
              "      <td>47.7694</td>\n",
              "      <td>-122.124</td>\n",
              "      <td>2550</td>\n",
              "      <td>40089</td>\n",
              "      <td>3.7500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2b1a30e-0a1d-4c96-be3e-09c78fa3840a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d2b1a30e-0a1d-4c96-be3e-09c78fa3840a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d2b1a30e-0a1d-4c96-be3e-09c78fa3840a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1e9387a6-433b-42d2-b76c-f9d2ba7a4626\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e9387a6-433b-42d2-b76c-f9d2ba7a4626')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1e9387a6-433b-42d2-b76c-f9d2ba7a4626 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9ded1052-76c7-4f3c-9609-be42a85e44de\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('raw_validation_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9ded1052-76c7-4f3c-9609-be42a85e44de button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('raw_validation_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "raw_validation_data"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "# Training data\n",
        "raw_training_data = pd.read_csv(train_path)\n",
        "raw_training_data\n",
        "# Validation data\n",
        "raw_validation_data = pd.read_csv(val_path)\n",
        "raw_validation_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQkAKzeSyMds"
      },
      "source": [
        "Perform the following preprocessing steps.\n",
        "1. remove the *ID* column from both training and validation data\n",
        "2. change *date* into 3 numerical features *day*, *month* and *year*, like in the warm up exercise\n",
        "3. The feature *yr_renovated* is set to 0 if the house has not been renovated. This creates an inconsistent meaning to the numerical values. Replace it with a new feature called *age_since_renovated*:\n",
        "\n",
        ">if *yr_renovate* != 0\n",
        ">> *age_since_renovated* = *year* - *yr\\_renovated*  \n",
        "\n",
        ">otherwise\n",
        ">> *age\\_since\\_renovated = year - yr\\_built*\n",
        "\n",
        "4. Normalize all the feautres using z-score normalization based on the training data. Do not normalize *price* as it is the target.\n",
        "To normalize a feature *x* using z-score normalization, the fomula is\n",
        "\n",
        ">$z=\\frac{x-\\mu}{\\sigma}$\n",
        "\n",
        "where $\\mu$ and $\\sigma$ are the mean and standard deviation of $x$ respectively. The normalized feature will have zero mean and unit standard deviation. Note that you should estimate $\\mu$ and $\\sigma$ for each feature only using the training data and use the same $\\mu$ and $\\sigma$ to normalize the features for both training and validation data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foN0WXP43pdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12ec9a6-aa59-47cd-efaa-27237975a904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of features 21\n",
            "\n",
            "Validation data after normalization:\n",
            "       bedrooms  bathrooms  sqft_living  sqft_lot    floors  waterfront  \\\n",
            "0     0.649956  -1.465613    -0.611163 -0.125186 -0.002300   -0.082432   \n",
            "1    -0.388520   0.169158    -0.304313  0.020425 -0.922332   -0.082432   \n",
            "2    -0.388520  -0.811705    -0.326231 -0.239710 -0.922332   -0.082432   \n",
            "3    -1.426996  -1.465613    -0.918013 -0.206307 -0.922332   -0.082432   \n",
            "4    -0.388520   0.169158     0.123085 -0.148234 -0.922332   -0.082432   \n",
            "...        ...        ...          ...       ...       ...         ...   \n",
            "1995  2.726909  -1.465613    -0.775547 -0.237801 -0.002300   -0.082432   \n",
            "1996 -0.388520  -0.484751    -0.753629 -0.182472 -0.922332   -0.082432   \n",
            "1997  1.688433   1.803929     1.777884 -0.190679  0.917732   -0.082432   \n",
            "1998 -0.388520   0.496112    -0.896095 -0.332712  2.757797   -0.082432   \n",
            "1999 -1.426996  -1.465613    -0.260477  1.594857 -0.922332   -0.082432   \n",
            "\n",
            "          view  condition     grade  sqft_above  ...  age_since_renovated  \\\n",
            "0    -0.304487   0.888924 -0.563015   -0.322821  ...             0.068634   \n",
            "1    -0.304487  -0.634184  0.290361   -0.539319  ...            -0.415923   \n",
            "2     5.014070   0.888924 -0.563015   -0.912178  ...             0.518580   \n",
            "3     1.025152   0.888924 -0.563015   -0.876095  ...             1.210806   \n",
            "4    -0.304487  -0.634184 -0.563015   -0.479181  ...            -0.485146   \n",
            "...        ...        ...       ...         ...  ...                  ...   \n",
            "1995 -0.304487  -0.634184 -1.416392   -0.803929  ...             1.487696   \n",
            "1996 -0.304487   0.888924 -0.563015   -0.479181  ...            -0.069811   \n",
            "1997 -0.304487  -0.634184  1.143738    2.299217  ...            -1.385039   \n",
            "1998 -0.304487  -0.634184  0.290361   -0.635541  ...            -1.177371   \n",
            "1999 -0.304487  -0.634184 -1.416392    0.062066  ...             0.657025   \n",
            "\n",
            "       zipcode       lat      long  sqft_living15  sqft_lot15  SaleMonth  \\\n",
            "0    -0.834826  1.226067 -0.156953      -0.653303   -0.172715   0.447342   \n",
            "1    -0.759987 -1.291481  1.204269       0.018666    0.083587  -1.154455   \n",
            "2     0.736796 -0.314038 -0.298012      -0.302710   -0.179520   1.728779   \n",
            "3     0.886474 -0.079596 -1.151421      -1.091544   -0.216946  -0.193377   \n",
            "4    -1.452249 -2.142687 -0.213377      -0.726343   -0.136445   1.088060   \n",
            "...        ...       ...       ...            ...         ...        ...   \n",
            "1995  0.512279 -0.263543 -0.960991      -1.412920   -0.261857  -1.795174   \n",
            "1996 -0.890955 -1.024578  0.223907      -0.770167   -0.169312  -0.513736   \n",
            "1997 -0.086435  0.234918  1.105528       1.727805   -0.212625  -1.474814   \n",
            "1998  0.530988  0.827155 -1.221951      -0.974679   -0.349639   1.088060   \n",
            "1999 -0.123854  1.508119  0.632979       0.822108    0.929286  -1.474814   \n",
            "\n",
            "       SaleDay  SaleYear   price  \n",
            "0    -1.142248 -0.682211  3.8900  \n",
            "1     0.019532  1.465639  3.3562  \n",
            "2    -0.445180 -0.682211  4.2000  \n",
            "3     0.251889 -0.682211  5.5900  \n",
            "4     1.646026 -0.682211  1.9425  \n",
            "...        ...       ...     ...  \n",
            "1995  0.484245  1.465639  4.1500  \n",
            "1996 -0.909892 -0.682211  2.4600  \n",
            "1997 -0.561358  1.465639  8.8995  \n",
            "1998  0.948957 -0.682211  4.4500  \n",
            "1999  1.297491  1.465639  3.7500  \n",
            "\n",
            "[2000 rows x 22 columns]\n"
          ]
        }
      ],
      "source": [
        "# 1 - Removing ID column\n",
        "training_data_without_id = raw_training_data.drop(columns=['id'])\n",
        "validation_data_without_id = raw_validation_data.drop(columns=['id'])\n",
        "\n",
        "# 2 - Change date into 3 numerical features\n",
        "def fix_date(df):\n",
        "  \"\"\"Changes the format mm/dd/yy into three columns 'SaleDay', 'SaleMonth', and 'SaleYear\"\"\"\n",
        "  df['date'] = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
        "  # extract month, day, and year into separate columns\n",
        "  df['SaleMonth'] = df['date'].dt.month\n",
        "  df['SaleDay'] = df['date'].dt.day\n",
        "  df['SaleYear'] = df['date'].dt.year\n",
        "  # drop the original date column\n",
        "  df = df.drop(columns=['date'])\n",
        "  return df\n",
        "\n",
        "training_data_without_id = fix_date(training_data_without_id)\n",
        "validation_data_without_id = fix_date(validation_data_without_id)\n",
        "\n",
        "\n",
        "# 3 - Age since renovation\n",
        "def fix_yr_renovation(df):\n",
        "  \"\"\"Replaces the feature 'yr_renovated' with a new feature 'age_since_renovated'\"\"\"\n",
        "  # Apply condition\n",
        "  df['age_since_renovated'] = df.apply(\n",
        "                                       lambda x: x['SaleYear'] - x['yr_renovated']\n",
        "                                       if x['yr_renovated'] != 0\n",
        "                                       else x['SaleYear'] - x['yr_built'],\n",
        "                                       axis=1)\n",
        "  cols = len(df.columns)\n",
        "  #print('Training columns', cols)\n",
        "  # move price column to the end\n",
        "  df.insert(cols-1, 'price', df.pop('price'))\n",
        "  # remove feature 'yr_renovated' and replace it with 'age_since_renovated'\n",
        "  column_index = df.columns.get_loc('yr_renovated')\n",
        "  df.pop('yr_renovated')\n",
        "  df.insert(column_index, 'age_since_renovated', df.pop('age_since_renovated'))\n",
        "  df.iloc[:,-20:]     # simply to display the last 20 columns and double check\n",
        "  return df\n",
        "\n",
        "training_data_without_id = fix_yr_renovation(training_data_without_id)\n",
        "validation_data_without_id = fix_yr_renovation(validation_data_without_id)\n",
        "\n",
        "\n",
        "# 4 - Feature Z-Score Normalization\n",
        "def zscore_normalize(training_df, validation_df):\n",
        "  \"\"\"Aplies z-score to training dataset and validation dataset\n",
        "     Heads Up: a. Mean and STD parameters from training dataset are used to normalize both\n",
        "                  training AND validation dataset\n",
        "               b. Only features are normalized, and target 'price' is not.\n",
        "  \"\"\"\n",
        "  normalized_training_df = training_df.copy().astype(float) # to avoid warnings from changing dtype (from int to float)\n",
        "  normalized_validation_df = validation_df.copy().astype(float) # to avoid warnings from changing dtype (from int to float)\n",
        "  cols = len(training_df.columns)\n",
        "  print('\\nNumber of features', cols-1)\n",
        "\n",
        "  for i in range(cols-1):\n",
        "    training_data_column = training_df.iloc[:,i]\n",
        "    validation_data_column = validation_df.iloc[:,i]\n",
        "    # Obtain Mean and STD from training dataset\n",
        "    mean = training_data_column.mean()\n",
        "    std = training_data_column.std()\n",
        "    # Normalize Training Data set\n",
        "    normalized_data_column = (training_data_column - mean)/std\n",
        "    normalized_training_df.iloc[:,i] = normalized_data_column\n",
        "    # Normalize Training Data set\n",
        "    normalized_data_column = (validation_data_column - mean)/std\n",
        "    normalized_validation_df.iloc[:,i] = normalized_data_column\n",
        "\n",
        "  return normalized_training_df, normalized_validation_df\n",
        "\n",
        "normalized_training_data, normalized_validation_data = zscore_normalize(training_data_without_id, validation_data_without_id)\n",
        "print('\\nValidation data after normalization:\\n', normalized_validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmnQIMbV6nkq"
      },
      "source": [
        "Let's do a quick testing of your normalization, please\n",
        "1. Estimate and print the new mean and standard deviation of the normalized features for the training data --- this should be 0 and 1 respectively.  \n",
        "2. Estimate and print the new mean and standard deviation of the normalized features for the validation data --- these values will not be 0 and 1, but somewhat close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIGpj-8v5aFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086f7ef8-fada-40cc-dc46-4fb651a8b544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data\n",
            "Size: 8000\n",
            "bedrooms: Mean = 0.000, Std = 1.000\n",
            "bathrooms: Mean = -0.000, Std = 1.000\n",
            "sqft_living: Mean = 0.000, Std = 1.000\n",
            "sqft_lot: Mean = 0.000, Std = 1.000\n",
            "floors: Mean = 0.000, Std = 1.000\n",
            "waterfront: Mean = 0.000, Std = 1.000\n",
            "view: Mean = -0.000, Std = 1.000\n",
            "condition: Mean = 0.000, Std = 1.000\n",
            "grade: Mean = 0.000, Std = 1.000\n",
            "sqft_above: Mean = -0.000, Std = 1.000\n",
            "sqft_basement: Mean = 0.000, Std = 1.000\n",
            "yr_built: Mean = 0.000, Std = 1.000\n",
            "age_since_renovated: Mean = -0.000, Std = 1.000\n",
            "zipcode: Mean = 0.000, Std = 1.000\n",
            "lat: Mean = -0.000, Std = 1.000\n",
            "long: Mean = 0.000, Std = 1.000\n",
            "sqft_living15: Mean = 0.000, Std = 1.000\n",
            "sqft_lot15: Mean = -0.000, Std = 1.000\n",
            "SaleMonth: Mean = -0.000, Std = 1.000\n",
            "SaleDay: Mean = -0.000, Std = 1.000\n",
            "SaleYear: Mean = 0.000, Std = 1.000\n",
            "\n",
            "Validation data\n",
            "Size: 2000\n",
            "bedrooms: Mean = 0.006, Std = 0.893\n",
            "bathrooms: Mean = -0.012, Std = 1.003\n",
            "sqft_living: Mean = 0.014, Std = 0.994\n",
            "sqft_lot: Mean = 0.005, Std = 0.913\n",
            "floors: Mean = 0.023, Std = 0.993\n",
            "waterfront: Mean = 0.015, Std = 1.088\n",
            "view: Mean = 0.003, Std = 1.026\n",
            "condition: Mean = -0.055, Std = 0.976\n",
            "grade: Mean = 0.057, Std = 1.034\n",
            "sqft_above: Mean = 0.028, Std = 0.997\n",
            "sqft_basement: Mean = -0.025, Std = 0.981\n",
            "yr_built: Mean = 0.018, Std = 1.000\n",
            "age_since_renovated: Mean = -0.024, Std = 0.999\n",
            "zipcode: Mean = -0.031, Std = 1.006\n",
            "lat: Mean = -0.019, Std = 1.001\n",
            "long: Mean = 0.016, Std = 0.987\n",
            "sqft_living15: Mean = 0.052, Std = 1.052\n",
            "sqft_lot15: Mean = -0.005, Std = 0.785\n",
            "SaleMonth: Mean = -0.018, Std = 0.984\n",
            "SaleDay: Mean = -0.017, Std = 1.008\n",
            "SaleYear: Mean = 0.009, Std = 1.004\n"
          ]
        }
      ],
      "source": [
        "# Normalization test\n",
        "def test_normalized_data(df):\n",
        "  \"\"\" Simple method to check the mean and std of each column of a dataframe \"\"\"\n",
        "\n",
        "  print(\"Size:\", df.shape[0])\n",
        "  cols = len(df.columns)\n",
        "  for i in range(cols-1):  # all columns but the target(price)\n",
        "    feature = df.columns[i]\n",
        "    mean = df.iloc[:,i].mean()\n",
        "    std = df.iloc[:,i].std()\n",
        "    print(f'{feature}: Mean = {mean:.3f}, Std = {std:.3f}')\n",
        "\n",
        "# Training Data\n",
        "print(\"Training data\")\n",
        "test_normalized_data(normalized_training_data)\n",
        "\n",
        "# Validation Data\n",
        "print(\"\\nValidation data\")\n",
        "test_normalized_data(normalized_validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_ifN_sv66Lt"
      },
      "source": [
        "\n",
        "## ***Question***\n",
        "Why is it import to use the same $\\mu$ and $\\sigma$ to perform normalization on the training and validation data? What would happen if we use $\\mu$ and $\\sigma$ estimated using the validation to perform normalization on the validation data?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqL-ygV7ewdv"
      },
      "source": [
        "**Your answer goes here:**\n",
        "By using the mean and std from the training data to z-score normalize both training and validation datasets helps to avoid overfitting and to increase how the model generalizes.\n",
        "\n",
        "\n",
        "Source: https://www.datacamp.com/tutorial/normalization-in-machine-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4djL2J-By8Y"
      },
      "source": [
        "# Part 1 (15 pts) Generate closed-form solution for reference.\n",
        "Our data now contains 21 numeric features, before we learn a linear regression model using gradient descent, we will first build the closed-form solution as a reference point. So for this part, you need to\n",
        "1. Implement the close-form solution for linear regression and apply it to the training data to learn the weight vector for your linear regression model. For matrix inversion you can use existing numpy functions. Specifically, we recommend the numpy.linalg.pinv() function for inverting near-singular matrices.\n",
        "2. Apply your learned linear regression model to the training data to make predictions for all training examples and report the Mean Squared Error.\n",
        "3. Apply your learned linear regression model to the validation data to make predictions for all the validation examples and report the mean squared error for the validation data.\n",
        "\n",
        "Your code should print the weight vector, which has 22 dimensions, one for each feature plus one additional $w_0$.  Your code should also report the MSE for the training and validation data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc2kX8JxXLiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e52d8d3-7449-4843-a407-90e0db06b547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset input size: (8000, 21)\n",
            "Training dataset output size: (8000,)\n",
            "Training dataset input size after adding x0: (8000, 22)\n",
            "Validation dataset input size: (2000, 21)\n",
            "Validation dataset output size: (2000,)\n",
            "Validation dataset input size after adding x0: (2000, 22)\n",
            "\n",
            "Weight of each feature:\n",
            "Bias is 5.362\n",
            "bedrooms, Weight: -0.281\n",
            "bathrooms, Weight: 0.339\n",
            "sqft_living, Weight: 0.763\n",
            "sqft_lot, Weight: 0.058\n",
            "floors, Weight: 0.018\n",
            "waterfront, Weight: 0.328\n",
            "view, Weight: 0.447\n",
            "condition, Weight: 0.200\n",
            "grade, Weight: 1.115\n",
            "sqft_above, Weight: 0.756\n",
            "sqft_basement, Weight: 0.155\n",
            "yr_built, Weight: -0.883\n",
            "age_since_renovated, Weight: -0.103\n",
            "zipcode, Weight: -0.263\n",
            "lat, Weight: 0.837\n",
            "long, Weight: -0.304\n",
            "sqft_living15, Weight: 0.144\n",
            "sqft_lot15, Weight: -0.099\n",
            "SaleMonth, Weight: 0.055\n",
            "SaleDay, Weight: -0.051\n",
            "SaleYear, Weight: 0.174\n",
            "\n",
            "Closed-form weights vector:\n",
            "  [ 5.36167284 -0.28135266  0.3390716   0.76341998  0.05815041  0.01813676\n",
            "  0.3281388   0.44675376  0.1998432   1.11544343  0.75623295  0.15546155\n",
            " -0.88336171 -0.10255779 -0.26341874  0.83661248 -0.30369641  0.14358099\n",
            " -0.09927428  0.05485035 -0.05063652  0.17375019]\n",
            "\n",
            "Predictions with training data: [2.31731748 5.35142825 4.34174302 ... 4.06965212 5.9711971  5.1361314 ]\n",
            "MSE with training data:  3.757887089954586\n",
            "\n",
            "Predictions with validation data: [4.42133582 3.49985748 6.09664119 ... 8.69175936 4.18978685 4.86608517]\n",
            "MSE with validation data:  4.503508105356856\n"
          ]
        }
      ],
      "source": [
        "# 0 - Turn DataFrames into X matrix and Y vector\n",
        "def df_into_X_and_Y(df, label):\n",
        "  \"\"\" Convenient method to convert datafram into numpy arrays and extract X, and Y\"\"\"\n",
        "\n",
        "  n_cols = df.shape[1]\n",
        "  #print(n_cols)\n",
        "  X = df.iloc[:, :-1].to_numpy()\n",
        "  print(f\"{label} input size: {X.shape}\")\n",
        "  #print(X)\n",
        "  Y = df.iloc[:, -1].to_numpy()\n",
        "  print(f\"{label} output size: {Y.shape}\")\n",
        "\n",
        "  # Adding x0 as a column of ones\n",
        "  ones_column = np.ones((X.shape[0], 1))\n",
        "  # Add the column of ones to the original array\n",
        "  X = np.hstack((ones_column, X))\n",
        "  print(f\"{label} input size after adding x0: {X.shape}\")\n",
        "\n",
        "  #print(Y)\n",
        "  return X, Y\n",
        "\n",
        "train_X, train_Y = df_into_X_and_Y(normalized_training_data, 'Training dataset')\n",
        "val_X, val_Y = df_into_X_and_Y(normalized_validation_data, 'Validation dataset')\n",
        "\n",
        "# 1 - Weight Vector: w = (X^T.X)^(-1).X^T.Y\n",
        "tranposed_X = train_X.T\n",
        "pseudo = np.dot(tranposed_X, train_X)\n",
        "pseudo = np.linalg.pinv(pseudo)           # Suggested\n",
        "weights = np.dot(pseudo, tranposed_X)\n",
        "weights = np.dot(weights, train_Y)\n",
        "\n",
        "features = normalized_training_data.columns.tolist()\n",
        "n_cols = train_X.shape[1]\n",
        "print('\\nWeight of each feature:')\n",
        "for i in range(n_cols):\n",
        "  if i == 0:\n",
        "    print(f'Bias is {weights[i]:.3f}')\n",
        "  else:\n",
        "      print(f'{features[i-1]}, Weight: {weights[i]:.3f}')\n",
        "print(\"\\nClosed-form weights vector:\\n \", weights)\n",
        "\n",
        "# 2 - Make predictions with the training data, and report MSE\n",
        "def MSE(predictions, ground_truth):\n",
        "  \"\"\"Function to calculate the Mean Squared Error\"\"\"\n",
        "  mse = np.mean((predictions - ground_truth)**2)\n",
        "  return mse\n",
        "\n",
        "train_predictions = np.dot(train_X, weights)\n",
        "print('\\nPredictions with training data:', train_predictions)\n",
        "training_MSE = MSE(train_predictions, train_Y)\n",
        "print('MSE with training data: ', training_MSE)\n",
        "\n",
        "# 3 - Make predictions with the validation data, and report MSE\n",
        "val_predictions = np.dot(val_X, weights)\n",
        "print('\\nPredictions with validation data:', val_predictions)\n",
        "val_MSE = MSE(val_predictions, val_Y)\n",
        "print('MSE with validation data: ', val_MSE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIn4qhdOftK9"
      },
      "source": [
        "## Question\n",
        "The learned feature weights are often used  to understand the importance of the features. The sign of the weights indicates if a feature positively or negatively impact the price, and the magnitude suggests the strength of the impact. Does the sign of all the features match your expection based on your common-sense understanding of what makes a house expensive? Please hightlight any surprises from the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l6zfscrfBgs"
      },
      "source": [
        "**Your answer goes here**\n",
        "Certainly not, some features' signs had the opposed expected effect on the price:\n",
        "* The biggest surprise is **Bedrooms**, with a weight of **-0.281**. In my opinion it should be the opposite.\n",
        "* Similarly I wasn't expecting any influence of **zip code**. However, it also has a negative influence with a weight of **-0.263**.\n",
        "* I also thought that **Latitude** would have a negative influence, but it had a positive one of **0.837**. This is quite a surprise, because in my opinion higher latitudes are colder, and people often prefer warmer weathers.\n",
        "* Lastly, I didn't expect much influence from **Longitude**, however it does have a strong negative influence of **-0.304**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhGbxDUIWUtZ"
      },
      "source": [
        "# Part 2 (40 pts) Implement and experiment with batch gradient descent\n",
        "\n",
        "Your implementation should take following inputs:\n",
        "\n",
        "1. the training data (with $d$ features and 1 target variable $y$),\n",
        "\n",
        "2. the learning rate $\\gamma$,\n",
        "\n",
        "3. the number of iterations $T$\n",
        "\n",
        "4. Optional convergence threshold (optional) $\\epsilon_l$ for the loss or $\\epsilon_g$ for the norm of the gradient\n",
        "\n",
        "It should output:\n",
        "1. the learned $d+1$ - dimensional weight vector\n",
        "2. the sequence of $T$ MSE losses, one for each training epoch. You will be asked to plot the losses as a function of training epoch later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3SKfBArbO2Y"
      },
      "outputs": [],
      "source": [
        "# print(train_X.shape, train_Y.shape)                   # for debugging\n",
        "\n",
        "# train_Y.reshape(-1,1)\n",
        "train_Y_reshaped = train_Y[:, np.newaxis]\n",
        "# print('reshape of Y', train_Y.shape)                    # for debugging\n",
        "\n",
        "# Batch Gradient Descent for MSE\n",
        "def batch_gd(X, y, lr, T, e_loss):\n",
        "  \"\"\"\n",
        "  Batch Gradient Descent\n",
        "  X: Training data\n",
        "  y: Ground truth\n",
        "  lr: learning rate\n",
        "  T: Number of iterations\n",
        "  e_loss : epsilon for the loss\n",
        "  \"\"\"\n",
        "\n",
        "  n_features = X.shape[1]\n",
        "  n_samples = X.shape[0]\n",
        "  gd_weights = np.zeros((n_features, 1))\n",
        "  gd_weights_gradient = np.zeros((n_features,1))\n",
        "  #print(gd_weights.shape)                                  # for debugging\n",
        "\n",
        "  current_loss = 1e5\n",
        "  loss_list = []      # keep track of loss\n",
        "  ctr = 0\n",
        "  while ctr < T:\n",
        "    # Computing the gradient\n",
        "    predictions = np.dot(X, gd_weights)\n",
        "    difference = predictions - y\n",
        "    #print('difference size', difference.shape)             # for debugging\n",
        "    gradient = (2/n_samples) * X.T @ difference\n",
        "    #print('gradient shape', gradient.shape)                # for debugging\n",
        "\n",
        "    # Update of weights\n",
        "    gd_weights = gd_weights - lr * gradient\n",
        "\n",
        "    # MSE loss\n",
        "    mse = MSE(predictions, train_Y_reshaped)\n",
        "    loss_list.append(mse)\n",
        "    # Check threshold\n",
        "    if abs(current_loss - mse) < e_loss or abs(current_loss - mse) > 1e6:\n",
        "      break\n",
        "    current_loss = mse\n",
        "\n",
        "    ctr += 1\n",
        "  print(f'Last iteration MSE: {mse:.2f}')\n",
        "\n",
        "  return gd_weights, loss_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4kKoNUIbi7r"
      },
      "source": [
        "You will now experiment with the batch gradient descent algorithm with different learning rate on the provided data.\n",
        "\n",
        "Please train your model for up to 3000 iterations using different learning rate: $\\gamma=10^{-i}$, $i=0,1,...,4$. For each learning rate, you can opt to stop early if it has converged (using the convergence threshold) or diverged (the loss start to grow larger).\n",
        "\n",
        "For each converging (not necessarily converged yet) learning rate, please compute and report the MSE of the final learned weights on the validation data.\n",
        "\n",
        "Also please provide a plot that shows the training loss curves (MSE as a function of the # of epochs) for all the converging learning rates. Please use different colors mark different learning rates and provide proper legends for your figure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zigtq7OhbQDR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "cd87c21a-7360-4471-b3a0-160574a6c6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Learning Rate  1\n",
            "Last iteration MSE: 4371032.823\n",
            "MSE with validation data:  477714576.31\n",
            "\n",
            "Learning Rate  0.1\n",
            "Last iteration MSE: 3.775\n",
            "MSE with validation data:  23.44\n",
            "\n",
            "Learning Rate  0.01\n",
            "Last iteration MSE: 3.822\n",
            "MSE with validation data:  23.32\n",
            "\n",
            "Learning Rate  0.001\n",
            "Last iteration MSE: 4.079\n",
            "MSE with validation data:  23.34\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'MSE')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Varying learning rate\n",
        "\n",
        "for i in range(4):\n",
        "\n",
        "  # Different learning rate\n",
        "  lr = 10 ** -i\n",
        "  print('\\nLearning Rate ', lr)\n",
        "\n",
        "  # Model training\n",
        "  gd_weights, loss_seq = batch_gd(train_X, train_Y_reshaped, lr, T= 3000, e_loss = 1e-3)\n",
        "\n",
        "  # Computation of MSE on the validation Data\n",
        "  gd_val_predictions = np.dot(val_X, gd_weights)\n",
        "  val_MSE = MSE(gd_val_predictions, val_Y)\n",
        "  print('MSE with validation data: ', round(val_MSE,2))\n",
        "\n",
        "  # Plots\n",
        "  if val_MSE < 1e4:\n",
        "    plt.plot(loss_seq, label = 'lr=' + str(lr))\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('Number of Epochs (iterations)')\n",
        "plt.ylabel('MSE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLewEsdZbShd"
      },
      "source": [
        "## Question\n",
        "\n",
        "Which learning rate leads to the best trainng and validation MSE respectively? Do you observe better training MSE tend to correpsond to better validation MSE? How is this different from the trend shown on page 51 of the lecture slides (titled danger of using training loss to select M) regarding overfitting? Is there any issue with using training loss to pick learning rate in this case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH2gZ8aLkRq9"
      },
      "source": [
        "**Learning rates vs best losses?**\n",
        "\n",
        "* For the training set, the best MSE was obtained with a lr of **0.1**.\n",
        "* For the validation set, the best MSE was obtained with a lr of **0.01**\n",
        "\n",
        "**Do you observer better training MSE ten to correspond to better validation MSE?**\n",
        "Not necessarily, it didn't happen in this case\n",
        "\n",
        "**How is this different from the trend shown on page 51**\n",
        "Slide 51 is about using the training loss to pick the M-th order of the polynominal. However\n",
        "\n",
        "Slide 51 is about how does the M-th order polynominal affect the fitting as a linear regression. The higher M, the higher the overfitting.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_FelVO1km4e"
      },
      "source": [
        "# Part 3. More exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPga1ENbSkm1"
      },
      "source": [
        "## **3(a). (20 pts) Normalization of features: what is the impact?**\n",
        "In part 1, you were asked to perform z-score normalization of all the features. In this part, we will ask you to first conceptually think about what is the impact this operation on the solution and then use some experiments to varify your conceptual understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPYb-oTeFt8B"
      },
      "source": [
        "### **Questions.**\n",
        "\n",
        "The normalization process applies a linear transformation to each feature, where the transformed feature $x'$ is simply a linear function of original feature $x$: $x'=\\frac{x-\\mu}{\\sigma}$.\n",
        "\n",
        "Let's disect the influence of this transformation on our learned linear regression model.\n",
        "1. How do you think this transformation will influnce the training and validation MSE we get for the closed-form solution? Why?\n",
        "2. How do you think this will change the magnitude of the weights of the learned model? Why?\n",
        "3. How do you think this will change the convergence behavior of the batch gradient descent algorithm? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liLbn0htPA3v"
      },
      "source": [
        "**Your answer goes here.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uti32GaMPGwD"
      },
      "source": [
        "Now please perform the following experiments to verify your answer to the above questions.\n",
        "1. Apply closed-form solution to data that did not go through the feature normalization step, and report the learned weights and the resulting training and testing MSEs.\n",
        "2. Apply gradient descent algorithm to data that did not go through the feature normalization step using different learning rate. Note that the learning rate used in previous section will no longer work here. You will need to search for an appropriate learning rate to get some converging behavior. Plot your MSE loss curve as a function of the epochs once you identify a convergent learning rate.\n",
        "Hint: the learning rate needs to be much, much, much, much, much smaller (think about each much as an order of manitude) than what was used in part 2). Also unless you let it run for a long time, it is unlikely to converge to the same level of loss values. So use a upper bound on the # of iterations so that it won't take forever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfqvAxFpkhAn"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqECgJNNQhzM"
      },
      "source": [
        "**Questions**\n",
        "\n",
        "Please revisit the questions above. Does your experiment confirm your expectation?  Can you provide explanations to the observed differences (or lack of differences) between the normalized data and unnormalized data? Based on these observations and your understanding of them, please comment on the benefits of normalizing the input features in learning for linear regressions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAiBYtRZROtt"
      },
      "source": [
        "**Your answer goes here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-sVWZX-SZzS"
      },
      "source": [
        "## **3(b). (20 pts) Explore the impact of correlated features**\n",
        "\n",
        "In the warm up exercise, you all have seen some features are highly correlated with one another. For example, there are multiple squared footage related features that are strongly correlated (e.g., *sqft_above* and *sqrt_living* has a correlation coefficient of 0.878).  This is referred to as multicollinearity phenomeon, where two or more features are correlated.\n",
        "\n",
        "There are numerous consequences from multicollinearity. It makes it more challenging to estimate the weights of the features accurately. The weights may become unstable, and their interpretation becomes less clear.\n",
        "\n",
        "In this part you will work with the pre-processed training set, and perform the following experiments **using the closed-form solution**.\n",
        "Specifically, your code should:\n",
        "\n",
        "1. Create five slighly different training sets, each of which is obtained by randomly subsample 75% of the orginial training set.\n",
        "2. Use the closed-form solution of linear regression to fit the model on each of the five training sets.\n",
        "3. For each model, report the learned weight vector in a table.\n",
        "The table should have five rows (one for each model) and a column for each featureâ€™s weight. Include a header row to clearly label the feature names for each column.\n",
        "4. Compute the variance of the learned weight coefficients across the five models for each feature. This variance will serve as a measure of the **stability** of the weight assigned to each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSoQZp6egyR2"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vodiypKgh26u"
      },
      "source": [
        "### **Questions**\n",
        "\n",
        "Ideally, we would like the weight coefficients to be stable across different runs, as this increases confidence in the model's reliability. Do highly correlated features tend to exhibit more instability in their weights across different training sets compared to less correlated features? Discuss any trend you observe based on the variance of the weight coefficients. How does the stability of these features relate to the multicollinearity issue present in this dataset?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xc4Aik1ikzE"
      },
      "source": [
        "**Your answer goes here.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej_IvVZEkS3j"
      },
      "source": [
        "# Bonus. In-class competition (5 bonus pts)\n",
        "\n",
        "We will host a in-class competition using the IA1 data, where you are encouraged to explore different ways to improve the prediction performance by manipunating the data. This could include: feature engineering such as removing, transforming features, constructing new features based on existing ones, using different encoding for the discrete features; data manipulation such as identifying and removing potential outliers; and target manipulation such as log transforming the price target. This is where you can get creative and test your ideas out.\n",
        "\n",
        "To participate in this competition, use the following link:\n",
        "https://www.kaggle.com/t/7a885211273e48968e3a5f1b556cb685\n",
        "\n",
        "You should continue working in the same team for this competition. The training and validation data provided on the kaggle site are the same as the IA1 assignment. To participate, you will need to train your model and apply it to testing data provided on kaggle, and submit prediction files to be scored.\n",
        "\n",
        "Your scoring will have two parts, the performance on the public leader board as well as the private leader board. The results on the public leader board is visible through out the competition so that you can gauge how well your model is performing in comparison to others. The private leader board shows the final evaluation performance and will be released only once after the competition is closed.\n",
        "\n",
        "Each team will be allowed to submit 3 final entries to be evaluated. You can use the public leaderboard performance to pick which models to use for your final evaluation entries.\n",
        "\n",
        "**Assginment of the bonus points:**\n",
        "\n",
        "**Performance bonus:** the top 3 teams on the **private** leader board will recieve 5 bonus points.\n",
        "\n",
        "**Participation bonus:** the 5 teams that submitted the most entries (with different performances) will recieve 3 bonus points. Also any team that participated the competition and got non-trivial performance will receive 2 bonus points.\n",
        "\n",
        "Bonus points are capped at 5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ISiBeSLtphi"
      },
      "source": [
        "**Please provide the team name on the kaggle competition here _________.\n",
        "Leave it blank if you opt not to participate.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBJ-Uzzk7yIC"
      },
      "outputs": [],
      "source": [
        "#running this code block will convert this notebook and its outputs into a pdf report.\n",
        "!jupyter nbconvert --to html /content/gdrive/MyDrive/Colab\\ Notebooks/IA1-2024.ipynb  # you might need to change this path to appropriate value to location your copy of the IA0 notebook\n",
        "\n",
        "input_html = '/content/gdrive/MyDrive/Colab Notebooks/IA1-2024.html' #you might need to change this path accordingly\n",
        "output_pdf = '/content/gdrive/MyDrive/Colab Notebooks/IA1output.pdf' #you might need to change this path or name accordingly\n",
        "\n",
        "# Convert HTML to PDF\n",
        "pdfkit.from_file(input_html, output_pdf)\n",
        "\n",
        "# Download the generated PDF\n",
        "files.download(output_pdf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}